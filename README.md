# M148FinalProject


## A. Main Document


### i. Dataset Description

The dataset used for this project is the Heart Failure Clinical Records dataset which contains 13 clinical features. The columns within the dataset include demographic information (age and sex), clinical measurements (ejection fraction, serum creatinine, serum sodium, and platelets), and medical conditions in patients (diabetes, smoking, anaemia, and high blood pressure). 


### ii. Overview of Problem

Cardiovascular diseases (CVDs) are the number 1 cause of death globally, claiming an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Heart failure is a common event associated with cardiovascular diseases. Most cardiovascular diseases can be prevented by addressing risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity, and harmful use of alcohol using population-wide strategies. Due to the multitude of risk factors for cardiovascular diseases and their toll on the body, early detection and treatment are of great importance. Using machine learning, we can pinpoint the greatest predictors of mortality due to cardiovascular disease to hopefully mitigate the occurrence of such. 


### iii. Key Methodology

The key methodology that worked to address this problem was a fully connected neural network. The neural network was chosen because it is able to learn and understand nonlinear interactions between medical features, which linear models may fail to capture and represent. Additionally, the problem we want to answer is a classification problem, which certain models like linear regression are not suitable for. So, we went with a neural network with sigmoid activation, since that model is suitable for classification and had the best performance compared to other classification methods we tried. For this particular dataset, risk features, such as serum creatinine levels and ejection fraction levels, may interact in complex ways to influence mortality, which made neural networks the best model. For the neural network, there was an input layer with 12 neurons corresponding to the clinical input features, two hidden layers one with 16 neurons and the other with 8 and both with ReLU activation for non-linearity, and an output layer with a single neuron using sigmoid activation. Before modeling, the numerical features were standardized, and the dataset was split into 75% training and 25% validation. The Adam optimizer was used with a learning rate of 0.001 to provide the most stability and a good pace of convergence. Training was conducted for 50 epochs, and accuracy and loss were observed for both training and validation to monitor convergence and watch for any signs of overfitting. 


### iv. Results, Conclusions, and Limitations

The model achieved a validation accuracy around 81.3%. The main evaluation metrics used were the binary cross-entropy loss which was tracked during training to evaluate convergence, accuracy which was primarily used to evaluate classification performance, and the ROC and AUC curves were computed to assess the ranking quality across the thresholds. The neural network was chosen as it gives great and extensive predictive performance while not failing to capture nonlinear dependencies between the predictors. However, the neural network does have limitations, including the need for precise care to the many hyperparameters such as learning rate, number of layers, and batch size. Another limitation of using neural networks is that they are less interpretable than other methods, which is an obstacle for clinical projects since interpretability is a key part to understanding our results. 


### v. Using the Code and Dataset

Upload the heart_failure_clinical_records_dataset.csv from the Github repository into the Final_Code.ipynb notebook. Install required libraries and run code. 




## B. Appendix 


### i. Exploratory Data Analysis

We first took a quick glance at the data to understand the variables involved and to determine if any cleaning was necessary. We sought to determine if there were any missing or duplicate values. Since there were none, there was minimal pre-processing required. We then looked at the distribution of mortality events to determine if there was any class imbalance, which would have affected the modeling strategy ahead. There were about twice the number of survivors (0) than deaths (1). Since the mortality is a bit imbalanced, we prioritised performance metrics such as ROC-AUC in addition to the accuracy of our models. In order to pinpoint possible predictors, we then examined summary statistics. We also grouped the features by death event and took the mean of the values for each feature to see if there were any large differences in the mean between survivors and deceased. Doing this, none of the features exhibited a significant difference in mean values between survivors and non-survivors. Being curious about the correlation between age and other possible predictors, we also plotted some scatterplots related to age and found no clear trends.

Next, we took a look at the variable distributions by plotting a histogram for each feature to understand the type of data and distribution within the data. This informed us on the feature scaling needed and prospective models. Features such as creatine_phosphokinase, platelets, and serum_sodium were highly skewed to the right. We also looked at the spread of each of the variables to determine which features may be strong predictors. Such features were ejection_fraction, serum_creatine, and time. This visualization informed us to use metrics beyond accuracy, possibly scale the continuous variables, and avoid assumptions of normality. We also visualised the data with density plots and boxplots that compared patient features by mortality, which helped to distinguish differences in measures of central tendencies and outliers between the groups. In the boxplots, we looked for features with non-overlapping IQRs and any outliers between survivors and decreases for each feature. Variables like age, ejection_fraction, serum_creatinine, serum_sodium, and time indicated higher between-groups variation. The addition of a correlation heatmap also was used to investigate predictor strength and multicollinearity, which is important for logistic regression. We looked for features with strong correlation with death_event and high inter-feature correlations. Time exhibited a moderate linear correlation with the death_event, while other variables ranged from no correlation to weak correlation.

We also investigated the pairwise relationships between the features to detect nonlinear patterns, threshold effects, overlap or separation between classes, and outliers. We found that deceased patients are concentrated at lower ejection fraction values and higher serum creatinine values. We also found that survivors tended to be younger, while those deceased were older. However, there was significant overlap between the two distributions, meaning that age might be a possible risk enhancer rather than a standalone predictor. We then investigated this further by creating an interactive scatterplot of Age vs. Ejection Fraction by mortality to determine if deaths cluster at low ejection fraction and high age. Examining the scatterplot, there was a fairly distinct boundary between samples with young age and high ejection fraction and old age and low ejection fraction, revealing a nonlinear interaction between the two. A Follow-up Time vs Serum Creatinine interactive scatterplot was also illuminating regarding the boundary in mortality rates given disease severity and treatment. We identified that deaths were correlated with shorter follow-up times and higher levels of creatinine, which is a key indicator of kidney health and function. This may imply rapid deterioration in patients with the risk factor of poor kidney health.

Additionally, we examined the outliers and scales within features. Creatinine_phosphokinase, serum_creatinine, and platelets had relatively higher outlier counts, while most other variables had none. Those outliers may indicate high-risk patients/extreme cases and ideally necessitate further context and research to understand them, and they were retained. Ultimately, they reemphasized to us that risk is nonlinear and that we should assess the model using ROC-AUC. From our feature scaling check, we saw that the standard deviations of each feature spanned five orders of magnitude, confirming our need to scale the model.

Thereafter, we split the dataset into 75% for training, which is large enough to ensure stable decision boundaries for our algorithms without sensitivity to the noise, and 25% for testing, which is large enough to create reliable and unbiased estimates of different evaluation metrics, such as accuracy and AUC. The training is used for tuning the model and hyperparameters. The testing was held out to measure the model’s success. Additionally, we set “stratify = y”, which stratifies the split by the death_event variable, preserving the original class distribution in both sets. This decreases the risk of class imbalance.


### ii. Data Pre-processing and Feature Engineering

In the feature scaling check, by examining the standard deviations of each feature, we found that feature scaling was required given that many of the features were on very different orders of magnitude from each other. This is especially important for models such as KNN, neural networks, and logistic regression. To be exact, we decided to standardize our continuous variables, and since the rest of our variables were in binary form one-hot encoding was not needed. We checked for missing values, however there were none in our dataset. Feature engineering was also minimized since the variables all had importance and contribution in their form to our project. 


### iii. Regression Analysis Method

Since our target variable, death_event, is binary, regression analysis was not a fruitful model. We attempted to use serum sodium to predict platelet counts instead. We scaled the platelet counts by dividing each value by 1000 to bring them to scale with the serum sodium. We performed both LS and LAD regression on the data and plotted a scatterplot of the data with the LAD and LS fitted lines, which showed evidence of overfitting and high bias. The models’ MAD and MAE were extremely high and the R2 was very low. However, since the target variable is binary the linear regression had large prediction errors and would produce values outside the appropriate probability range of 0 to 1. We see for both models that we get rMSE: 97.4519, MAE:  67, and MAD: 46. These are extremely high error rates, confirming that we need to move to a classification algorithm that models probability through the log-odds transformation. Hence, regularization was not needed and classification would suit the model better and be more reliable. 


### iv. Logistic Regression Method

We used logistic regression to predict death_event based on age, high blood pressure, serum creatinine, and ejection fraction. We trained and tested the model using a test size of 0.2 and a random state of 42. After training the model and predicting on the test set, we constructed a confusion matrix, which displayed an accuracy of 0.8033, an error of 0.1967, a TP rate of 0.5211, and a TN rate of 0.9226. We also performed 5-fold cross-validation and constructed a ROC curve for each fold. The average AUC of 5 folds was 0.7251. In our ROC curve, we chose a default threshold value of 0.5 for positive predictions. The reason we chose this default value is that it gives each possibility, living or dying, an equal chance. Additionally, our DEATH_EVENT variable is not an imbalanced variable, hence leaving 0.5 as an appropriate value. 

The selected clinical features, age, high blood pressure, serum creatinine, and ejection fraction, are meaningful predictors of mortality, as evidenced by a test accuracy of 0.803 and an average cross-validated AUC of 0.725. Logistic regression performed substantially better than random guessing, AUC = 0.5, indicating that mortality is related to these clinical variables rather than occurring randomly. The difference between the true negative rate, 0.9226, and true positive rate, 0.521, suggests that the model is much better at identifying survivors than deaths, given the class imbalance of death_events. Through logistic regression offers valuable feature importance insight given through model coefficients as they reflect the magnitude of the log-odds of death allowing identification of the important predictors. Moreover, to improve the stability and prevent overfitting for our model, we applied L2 regularization which helps minimize our coefficients and reduce the variance. The minimization of our coefficients and reduction of the variance provide consistent and trustworthy predictive performance regardless of skew or correlation amongst distributions and predictors. 

From our logistic regression analysis, we also learned the log odds coefficients and odds ratios of each predictor, which we were able to use for feature importance since we standardized the data it was calculated on. Our log odds coefficients were highest for age and serum creatine, at 0.709 and 0.737 respectively, indicating that higher age and higher serum creatine levels increases the log odds of death. The odds ratios for those predictors were 2.032 for age and 2.090 for serum creatine. This indicates that 1 standard deviation increase in age or serum creatine approximately doubles the odds of death. From these analyses, we were able to determine age and serum creatine as being the most important predictors.

In comparison, the log odds coefficient for high blood pressure was 0.210 and the odds ratio was 1.234, indicating it is a less important predictor. For each standard deviation increase in high blood pressure, the odds of death will increase by 23%. This is lower than the doubling (increase by 200% per standard deviation) of the odds of death with the age and serum creatine features.
For ejection fraction, the log odds coefficient was negative (-0.83) and the odds ratio was less than 1 (0.436), indicating that 1 standard deviation increase in ejection fraction levels will decrease the odds of death by almost 50%. While the relationship between ejection fraction and mortality is useful to know if we want to prevent death, it is not an important feature for predicting the occurrence of death, which is our specific question for this report.


### v. KNN and Random Forest Methods

We used the same set of features and response variable chosen from the logistic regression model on the random forest and KNN models. We used a test train split of 0.2 and a random state of 42 with 100  n_estimators (decision trees). We then calculated the confusion matrix, prediction accuracy, prediction error, true positive rate, true negative rate, and F1 score on the training dataset. We formed a 5-fold cross-validation to compute the AUC score and accuracy. Our model had an AUC of 0.8034, demonstrating that our model performed well when distinguishing between those who lived and those who died. We also tested using 5-fold cross-validation to show how stable our model is. From this, we saw great predictive power. The mean accuracy was 0.6833, and mean AUC was 0.7027.


### vi. PCA Method

The unsupervised learning method we decided to use is PCA, since our dataset has a good distribution of both categorical and numeric variables, as this will help us most in performing dimensionality reduction. We standardized each variable since PCA is sensitive to scaling. We then applied PCA, calculating the portion of variability explained by each PC, constructing a scree plot displaying this. After constructing the scree plot, we chose the number of PCs that would explain about 90% of the variance, 10 PCs, and then refit the data with those chosen PCs, plotting the first two principal components. Plotting the first two PCs, there was no discernible distinction or separation between the two death events, alive or dead. Therefore, PCA has helped us learn that the structure in the data does not have a strong low-dimensional structure, indicating that it is complex and there is no clear separation between outcome classes.


### vii. Neural Network Method 

We developed a neural network to explore and dive into the prediction of the DEATH_EVENT variable. We implemented a multilayer network with an input layer of 12 neurons, two hidden layers one with 16 neurons and the other 8 and both with ReLU activation for non-linearity, and an output layer with a single neuron using sigmoid activation. Instead of simply relying on the Adam optimizer which is the default, we specified our learning rate to 0.001 in order to analyze the impact of the learning-rate selection. We experimented with different values but came to the conclusion that the learning rate of 0.001 was the one that provided the best AUC and accuracy score. In comparison to the random forest, the neural network had strong performance but required more precision and tuning and was harder to interpret. Batch processing was implemented with a batch size of 16 to minimize the validation loss. Early stopping was implemented to avoid overfitting, and the training stopped at Epoch 32. The final results of the attempt showed a final accuracy of around 81.3%, with the seed = 42.


### viii. Hyperparameter Tuning

One example of hyperparameter tuning applied to the project was playing around with different learning rates for our NN. We tested the learning rates on our neural network from largest to smallest to get the best accuracy and model generalisation. We tested learning rates from 0.1 to 0.001 with increments of 10 and found that 0.001 performed the best. Another one was the batch size which we set to 16 to minimize validation loss and to help with computational costs. Using larger batch sizes would result in slower convergence and longer computational times. After experimenting, we saw that smaller dimensions for the hidden layers would lead to underfitting while higher dimensions led to overfitting, hence our choices of 16 and 32 neurons allowed us to generalize with minimal errors. Lastly, we chose the Adam optimizer as it is more stable for tabular datasets and would help produce a smoother convergence. 
